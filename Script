################################################################
## Homework: Scraping assignment
## February 23rd, 2015
## Script: Collecting and Analyzing Twitter data
## Author: Riley Sullivan
################################################################


> setwd("/Users/Riley/Desktop")

## INSTALLING PACKAGES
> doInstall <- TRUE  # Change to FALSE if you don't want packages installed.
> toInstall <- c("ROAuth", "twitteR", "streamR", "ggplot2", "stringr",
+ 	"tm", "RCurl", "maps", "Rfacebook", "topicmodels", "devtools")
> if(doInstall){install.packages(toInstall, repos = "http://cran.r-project.org")}
trying URL 'http://cran.r-project.org/bin/macosx/mavericks/contrib/3.1/ROAuth_0.9.6.tgz'
Content type 'application/x-gzip' length 49703 bytes (48 Kb)
opened URL
==================================================
downloaded 48 Kb

trying URL 'http://cran.r-project.org/bin/macosx/mavericks/contrib/3.1/twitteR_1.1.8.tgz'
Content type 'application/x-gzip' length 435334 bytes (425 Kb)
opened URL
==================================================
downloaded 425 Kb

trying URL 'http://cran.r-project.org/bin/macosx/mavericks/contrib/3.1/streamR_0.2.1.tgz'
Content type 'application/x-gzip' length 41704 bytes (40 Kb)
opened URL
==================================================
downloaded 40 Kb

trying URL 'http://cran.r-project.org/bin/macosx/mavericks/contrib/3.1/ggplot2_1.0.0.tgz'
Content type 'application/x-gzip' length 2671874 bytes (2.5 Mb)
opened URL
==================================================
downloaded 2.5 Mb

trying URL 'http://cran.r-project.org/bin/macosx/mavericks/contrib/3.1/stringr_0.6.2.tgz'
Content type 'application/x-gzip' length 72797 bytes (71 Kb)
opened URL
==================================================
downloaded 71 Kb

trying URL 'http://cran.r-project.org/bin/macosx/mavericks/contrib/3.1/tm_0.6.tgz'
Content type 'application/x-gzip' length 652392 bytes (637 Kb)
opened URL
==================================================
downloaded 637 Kb

trying URL 'http://cran.r-project.org/bin/macosx/mavericks/contrib/3.1/RCurl_1.95-4.5.tgz'
Content type 'application/x-gzip' length 718964 bytes (702 Kb)
opened URL
==================================================
downloaded 702 Kb

trying URL 'http://cran.r-project.org/bin/macosx/mavericks/contrib/3.1/maps_2.3-9.tgz'
Content type 'application/x-gzip' length 2069999 bytes (2.0 Mb)
opened URL
==================================================
downloaded 2.0 Mb

trying URL 'http://cran.r-project.org/bin/macosx/mavericks/contrib/3.1/Rfacebook_0.5.tgz'
Content type 'application/x-gzip' length 60208 bytes (58 Kb)
opened URL
==================================================
downloaded 58 Kb

trying URL 'http://cran.r-project.org/bin/macosx/mavericks/contrib/3.1/topicmodels_0.2-1.tgz'
Content type 'application/x-gzip' length 1563585 bytes (1.5 Mb)
opened URL
==================================================
downloaded 1.5 Mb

trying URL 'http://cran.r-project.org/bin/macosx/mavericks/contrib/3.1/devtools_1.7.0.tgz'
Content type 'application/x-gzip' length 296811 bytes (289 Kb)
opened URL
==================================================
downloaded 289 Kb


The downloaded binary packages are in
	/var/folders/81/yxszbnyd42z2dm72d_y396k00000gn/T//RtmpBSSdxK/downloaded_packages

#####################################
### CREATING YOUR OWN OAUTH TOKEN ###
#####################################

## Step 1: go to apps.twitter.com and sign in
## Step 2: click on "Create New App"
## Step 3: fill name, description, and website (it can be anything, even google.com)
## Step 4: Agree to user conditions
## Step 5: copy consumer key and consumer secret and paste below

> library(ROAuth)
> requestURL <- "https://api.twitter.com/oauth/request_token"
> accessURL <- "https://api.twitter.com/oauth/access_token"
> authURL <- "https://api.twitter.com/oauth/authorize"
> consumerKey <- "odynvoC1hmth5VYIdBFubuGZq"
> consumerSecret <- "3BQZROHF1nDDBzYZgE3rZf85nczPRYeKA16tuXZEmiB4VCAXae"

> my_oauth <- OAuthFactory$new(consumerKey=consumerKey,
+   consumerSecret=consumerSecret, requestURL=requestURL,
+   accessURL=accessURL, authURL=authURL)

## run this line and go to the URL that appears on screen
> my_oauth$handshake(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl"))
To enable the connection, please direct your web browser to: 
https://api.twitter.com/oauth/authorize?oauth_token=JHnZF3ocwTyGFmsOe1sDgjogPTiZjLtC
When complete, record the PIN given to you and provide it here: 4628351

> access_token <- "239916125-iMQydlpS89jxHCDkyv99Rvfk5kFYn87nm8jJVojl"
> access_secret <- "3gG8Wwmv2Wt17CWSnXSvxpIbEroMtjk2jsiUXI1AB7sdA"
> setup_twitter_oauth(consumerKey, consumerSecret, access_token, access_secret)
[1] "Using direct authentication"
> 4628351
[1] 4628351

## testing that it works
> library(twitteR)
> searchTwitter('obama', n=1)
[[1]]
[1] "Alkazapper: White House Refuses to Call 21 BEHEADING Victims Christians while Egypt launches airstrikes on ... http://t.co/f3nEGotNKY"

## saving oauth token for use in future sessions with twitteR or streamR
> save(my_oauth, file="oauth_token.Rdata")

#############################################
### DOWNLOADING RECENT TWEETS FROM A USER ###
#############################################

## capturing the most recent tweets of @nytimes
> load("oauth_token.Rdata")
> tweets <- searchTwitter("nytimes", n=500)
> tweets <- twListToDF(tweets)

## Function written by Pablo Barbera to store JSON data
> source("functions.r")

## installing packages
> toInstall <- c("rjson")
> install.packages ("RJSONIO")
--- Please select a CRAN mirror for use in this session ---
trying URL 'http://watson.nci.nih.gov/cran_mirror/bin/macosx/mavericks/contrib/3.1/RJSONIO_1.3-0.tgz'
Content type 'application/octet-stream' length 1279592 bytes (1.2 Mb)
opened URL
==================================================
downloaded 1.2 Mb


The downloaded binary packages are in
	/var/folders/81/yxszbnyd42z2dm72d_y396k00000gn/T//RtmpBSSdxK/downloaded_packages
> install.packages ("rjson")
trying URL 'http://watson.nci.nih.gov/cran_mirror/bin/macosx/mavericks/contrib/3.1/rjson_0.2.15.tgz'
Content type 'application/octet-stream' length 160850 bytes (157 Kb)
opened URL
==================================================
downloaded 157 Kb


The downloaded binary packages are in
	/var/folders/81/yxszbnyd42z2dm72d_y396k00000gn/T//RtmpBSSdxK/downloaded_packages

## downloading tweets
> getTimeline(filename="tweets_nytimes.json", screen_name="nytimes", 
+     n=500, oauth=my_oauth, trim_user="false")
178  hits left
177  hits left
200 tweets. Max id:  566814167609655296 
176  hits left
400 tweets. Max id:  566319592587689985 
175  hits left
600 tweets. Max id:  565890649854332929 


> install.packages ("RCurl")
trying URL 'http://watson.nci.nih.gov/cran_mirror/bin/macosx/mavericks/contrib/3.1/RCurl_1.95-4.5.tgz'
Content type 'application/octet-stream' length 718964 bytes (702 Kb)
opened URL
==================================================
downloaded 702 Kb


The downloaded binary packages are in
	/var/folders/81/yxszbnyd42z2dm72d_y396k00000gn/T//RtmpBSSdxK/downloaded_packages

# it's stored in disk and I can read it with the 'parseTweets' function in
# the streamR package
> library(streamR)
> tweets <- parseTweets("tweets_nytimes.json")
600 tweets have been parsed. 

## capturing the most recent tweets of @WSJ
> timeline <- userTimeline('WSJ', n=500)
> timeline <- twListToDF(timeline)
> source("functions.r")
> getTimeline(filename="tweets_wsj.json", screen_name="wsj", 
+     n=500, oauth=my_oauth, trim_user="false")
170  hits left
169  hits left
200 tweets. Max id:  566471921962070016 
168  hits left
400 tweets. Max id:  565922337170944000 
167  hits left
600 tweets. Max id:  565422244961472512 
> library(streamR)
> tweets <- parseTweets("tweets_wsj.json")
600 tweets have been parsed. 

## capturing the most recent tweets of @CNN
> timeline <- userTimeline('CNN', n=500)
> timeline <- twListToDF(timeline)
> source("functions.r")
> getTimeline(filename="tweets_cnn.json", screen_name="cnn", n=500, oauth=my_oauth, trim_user="false")
162  hits left
161  hits left
200 tweets. Max id:  565867118013726721 
160  hits left
400 tweets. Max id:  564453469806145538 
159  hits left
600 tweets. Max id:  563253269137743872 
> library(streamR)
> tweets <- parseTweets("tweets_cnn.json")
600 tweets have been parsed. 

## capturing the most recent tweets of @FoxNews
> timeline <- userTimeline('FoxNews', n=500)
> timeline <- twListToDF(timeline)
> 
> source("functions.r")
> getTimeline(filename="tweets_fox.json", screen_name="fox", n=500, oauth=my_oauth, trim_user="false")
180  hits left
179  hits left
200 tweets. Max id:  562400298900021248 
178  hits left
400 tweets. Max id:  558559811080839168 
177  hits left
600 tweets. Max id:  553883229032574977 
> library(streamR)
> tweets <- parseTweets("tweets_fox.json")
600 tweets have been parsed. 

## capturing the most recent tweets of @washingtonpost
> timeline <- userTimeline('washingtonpost', n=500)
> timeline <- twListToDF(timeline)
> source("functions.r")
> getTimeline(filename="tweets_wapo.json", screen_name="washingtonpost", n=500, oauth=my_oauth, trim_user="false")
168  hits left
167  hits left
200 tweets. Max id:  566196695969849344 
166  hits left
400 tweets. Max id:  565556694135353345 
165  hits left
600 tweets. Max id:  564889532743811072 
> library(streamR)
> tweets <- parseTweets("tweets_wapo.json")
604 tweets have been parsed. 

## capturing the most recent tweets of @msnbc
> timeline <- userTimeline('msnbc', n=500)
> timeline <- twListToDF(timeline)
> source("functions.r")
> getTimeline(filename="tweets_msnbc.json", screen_name="msnbc", n=500, oauth=my_oauth, trim_user="false")
160  hits left
159  hits left
200 tweets. Max id:  566024037416304642 
158  hits left
400 tweets. Max id:  564931877954650112 
157  hits left
600 tweets. Max id:  563472172661100545 
> library(streamR)
> tweets <- parseTweets("tweets_msnbc.json")
600 tweets have been parsed. 

## capturing the most recent tweets of @USATODAY
> timeline <- userTimeline('USATODAY', n=500)
> timeline <- twListToDF(timeline)
> source("functions.r")
> getTimeline(filename="tweets_usatoday.json", screen_name="USATODAY", n=500, oauth=my_oauth, trim_user="false")
137  hits left
136  hits left
200 tweets. Max id:  565935253156794368 
135  hits left
400 tweets. Max id:  565178010731835392 
134  hits left
600 tweets. Max id:  564567126409625602 
> library(streamR)
> tweets <- parseTweets("tweets_usatoday.json")
600 tweets have been parsed. 

## capturing the most recent tweets of @nprnews
> timeline <- userTimeline('nprnews', n=500)
> timeline <- twListToDF(timeline)
> source("functions.r")
> getTimeline(filename="tweets_nprnews.json", screen_name="nprnews", n=500, oauth=my_oauth, trim_user="false")
129  hits left
128  hits left
200 tweets. Max id:  564951988560203776 
127  hits left
400 tweets. Max id:  562655868119154688 
126  hits left
600 tweets. Max id:  560453765883375616 
> library(streamR)
> tweets <- parseTweets("tweets_nprnews.json")
600 tweets have been parsed. 

## capturing the most recent tweets of @ABC
> timeline <- userTimeline('ABC', n=500)
> timeline <- twListToDF(timeline)
> source("functions.r")
> getTimeline(filename="tweets_abc.json", screen_name="ABC", n=500, oauth=my_oauth, trim_user="false")
121  hits left
120  hits left
200 tweets. Max id:  565604468860157952 
119  hits left
400 tweets. Max id:  564255281014902784 
118  hits left
600 tweets. Max id:  562819342794899456 
> library(streamR)
> tweets <- parseTweets("tweets_abc.json")
600 tweets have been parsed. 

## capturing the most recent tweets of @AP
> timeline <- userTimeline('AP', n=500)
> timeline <- twListToDF(timeline)
> source("functions.r")
> getTimeline(filename="tweets_ap.json", screen_name="AP", n=500, oauth=my_oauth, trim_user="false")
175  hits left
174  hits left
200 tweets. Max id:  566149234249195520 
173  hits left
400 tweets. Max id:  565389816121278464 
172  hits left
600 tweets. Max id:  564546685490450432 
> library(streamR)
> tweets <- parseTweets("tweets_ap.json")
600 tweets have been parsed. 

###############################################
### SENTIMENT ANALYSIS	###
###############################################

# Loading tweets previously downloaded
> tweets <- parseTweets("tweets_abc.json")
600 tweets have been parsed. 

# loading lexicon of positive and negative words (from Neal Caren)
> lexicon <- read.csv("lexicon.csv", stringsAsFactors=F)
> pos.words <- lexicon$word[lexicon$polarity=="positive"]
> neg.words <- lexicon$word[lexicon$polarity=="negative"]

# function to clean the text
> clean_tweets <- function(text){
+     # loading required packages
+     lapply(c("tm", "Rstem", "stringr"), require, c=T, q=T)
+     # avoid encoding issues by dropping non-unicode characters
+     utf8text <- iconv(text, to='UTF-8-MAC', sub = "byte")
+     # remove punctuation and convert to lower case
+     words <- removePunctuation(utf8text)
+     words <- tolower(words)
+     # spliting in words
+     words <- str_split(words, " ")
+     return(words)
+ }

# now I clean the text
> text <- clean_tweets(tweets$text)           

# a function to classify individual tweets
> classify <- function(words, pos.words, neg.words){
+     # count number of positive and negative word matches
+     pos.matches <- sum(words %in% pos.words)
+     neg.matches <- sum(words %in% neg.words)
+     return(pos.matches - neg.matches)
+ }

# this is how I would apply it
> classify(text[[1]], pos.words, neg.words)
[1] 0
> classify(text[[7]], pos.words, neg.words)
[1] 0

# but I want to aggregate over many tweets...
> classifier <- function(text, pos.words, neg.words){
+     # classifier
+     scores <- unlist(lapply(text, classify, pos.words, neg.words))
+     n <- length(scores)
+     positive <- as.integer(length(which(scores>0))/n*100)
+     negative <- as.integer(length(which(scores<0))/n*100)
+     neutral <- 100 - positive - negative
+     cat(n, "tweets:", positive, "% positive,",
+         negative, "% negative,", neutral, "% neutral")
+ }
> 
> # applying classifier function
> classifier(text, pos.words, neg.words)
600 tweets: 26 % positive, 37 % negative, 37 % neutral

# Loading tweets I will use
> tweets <- parseTweets("tweets_ap.json")
600 tweets have been parsed. 
> lexicon <- read.csv("lexicon.csv", stringsAsFactors=F)
> pos.words <- lexicon$word[lexicon$polarity=="positive"]
> neg.words <- lexicon$word[lexicon$polarity=="negative"]
> clean_tweets <- function(text){
+     # loading required packages
+     lapply(c("tm", "Rstem", "stringr"), require, c=T, q=T)
+     # avoid encoding issues by dropping non-unicode characters
+     utf8text <- iconv(text, to='UTF-8-MAC', sub = "byte")
+     # remove punctuation and convert to lower case
+     words <- removePunctuation(utf8text)
+     words <- tolower(words)
+     # spliting in words
+     words <- str_split(words, " ")
+     return(words)
+ }
> 
> # Cleaning the text
> text <- clean_tweets(tweets$text)

> # a function to classify individual tweets
> classify <- function(words, pos.words, neg.words){
+     # count number of positive and negative word matches
+     pos.matches <- sum(words %in% pos.words)
+     neg.matches <- sum(words %in% neg.words)
+     return(pos.matches - neg.matches)
+ }
> classifier <- function(text, pos.words, neg.words){
+     # classifier
+     scores <- unlist(lapply(text, classify, pos.words, neg.words))
+     n <- length(scores)
+     positive <- as.integer(length(which(scores>0))/n*100)
+     negative <- as.integer(length(which(scores<0))/n*100)
+     neutral <- 100 - positive - negative
+     cat(n, "tweets:", positive, "% positive,",
+         negative, "% negative,", neutral, "% neutral")
+ }
> 
> # applying classifying function
> classifier(text, pos.words, neg.words)
600 tweets: 20 % positive, 39 % negative, 41 % neutral

# Loading tweets
> tweets <- parseTweets("tweets_cnn.json")
600 tweets have been parsed. 
> lexicon <- read.csv("lexicon.csv", stringsAsFactors=F)
> pos.words <- lexicon$word[lexicon$polarity=="positive"]
> neg.words <- lexicon$word[lexicon$polarity=="negative"]     
> 
> # function to clean the text
> clean_tweets <- function(text){
+     # loading required packages
+     lapply(c("tm", "Rstem", "stringr"), require, c=T, q=T)
+     # avoid encoding issues by dropping non-unicode characters
+     utf8text <- iconv(text, to='UTF-8-MAC', sub = "byte")
+     # remove punctuation and convert to lower case
+     words <- removePunctuation(utf8text)
+     words <- tolower(words)
+     # spliting in words
+     words <- str_split(words, " ")
+     return(words)
+ }
> 
> # Clean the text
> text <- clean_tweets(tweets$text)

> # Function to classify individual tweets
> classify <- function(words, pos.words, neg.words){
+     # count number of positive and negative word matches
+     pos.matches <- sum(words %in% pos.words)
+     neg.matches <- sum(words %in% neg.words)
+     return(pos.matches - neg.matches)
+ }
> 
> # this is how I would apply it
> classify(text[[1]], pos.words, neg.words)
[1] 0
> classify(text[[7]], pos.words, neg.words)
[1] 0
> 
> # but I want to aggregate over many tweets...
> classifier <- function(text, pos.words, neg.words){
+     # classifier
+     scores <- unlist(lapply(text, classify, pos.words, neg.words))
+     n <- length(scores)
+     positive <- as.integer(length(which(scores>0))/n*100)
+     negative <- as.integer(length(which(scores<0))/n*100)
+     neutral <- 100 - positive - negative
+     cat(n, "tweets:", positive, "% positive,",
+         negative, "% negative,", neutral, "% neutral")
+ }
> 
> # applying classifier function
> classifier(text, pos.words, neg.words)
600 tweets: 27 % positive, 29 % negative, 44 % neutral

# Loading tweets I will use
> tweets <- parseTweets("tweets_fox.json")
600 tweets have been parsed. 
> lexicon <- read.csv("lexicon.csv", stringsAsFactors=F)
> pos.words <- lexicon$word[lexicon$polarity=="positive"]
> neg.words <- lexicon$word[lexicon$polarity=="negative"]

> # function to clean the text
> clean_tweets <- function(text){
+     # loading required packages
+     lapply(c("tm", "Rstem", "stringr"), require, c=T, q=T)
+     # avoid encoding issues by dropping non-unicode characters
+     utf8text <- iconv(text, to='UTF-8-MAC', sub = "byte")
+     # remove punctuation and convert to lower case
+     words <- removePunctuation(utf8text)
+     words <- tolower(words)
+     # spliting in words
+     words <- str_split(words, " ")
+     return(words)
+ }
> 
> # now I clean the text
> text <- clean_tweets(tweets$text)

> # a function to classify individual tweets
> classify <- function(words, pos.words, neg.words){
+     # count number of positive and negative word matches
+     pos.matches <- sum(words %in% pos.words)
+     neg.matches <- sum(words %in% neg.words)
+     return(pos.matches - neg.matches)
+ }
> 
> # this is how I would apply it
> classify(text[[1]], pos.words, neg.words)
[1] 0
> classify(text[[7]], pos.words, neg.words)
[1] 0
> 
> # but I want to aggregate over many tweets...
> classifier <- function(text, pos.words, neg.words){
+     # classifier
+     scores <- unlist(lapply(text, classify, pos.words, neg.words))
+     n <- length(scores)
+     positive <- as.integer(length(which(scores>0))/n*100)
+     negative <- as.integer(length(which(scores<0))/n*100)
+     neutral <- 100 - positive - negative
+     cat(n, "tweets:", positive, "% positive,",
+         negative, "% negative,", neutral, "% neutral")
+ }
> 
> # applying classifier function
> classifier(text, pos.words, neg.words)
600 tweets: 41 % positive, 12 % negative, 47 % neutral

# Loading tweets I will use
> tweets <- parseTweets("tweets_msnbc.json")
600 tweets have been parsed. 
> lexicon <- read.csv("lexicon.csv", stringsAsFactors=F)
> pos.words <- lexicon$word[lexicon$polarity=="positive"]
> neg.words <- lexicon$word[lexicon$polarity=="negative"]

> # function to clean the text
> clean_tweets <- function(text){
+     # loading required packages
+     lapply(c("tm", "Rstem", "stringr"), require, c=T, q=T)
+     # avoid encoding issues by dropping non-unicode characters
+     utf8text <- iconv(text, to='UTF-8-MAC', sub = "byte")
+     # remove punctuation and convert to lower case
+     words <- removePunctuation(utf8text)
+     words <- tolower(words)
+     # spliting in words
+     words <- str_split(words, " ")
+     return(words)
+ }
> 
> # now I clean the text
> text <- clean_tweets(tweets$text)

> # a function to classify individual tweets
> classify <- function(words, pos.words, neg.words){
+     # count number of positive and negative word matches
+     pos.matches <- sum(words %in% pos.words)
+     neg.matches <- sum(words %in% neg.words)
+     return(pos.matches - neg.matches)
+ }
> 
> # this is how I would apply it
> classify(text[[1]], pos.words, neg.words)
[1] 0
> classify(text[[7]], pos.words, neg.words)
[1] 0
> 
> # but I want to aggregate over many tweets...
> classifier <- function(text, pos.words, neg.words){
+     # classifier
+     scores <- unlist(lapply(text, classify, pos.words, neg.words))
+     n <- length(scores)
+     positive <- as.integer(length(which(scores>0))/n*100)
+     negative <- as.integer(length(which(scores<0))/n*100)
+     neutral <- 100 - positive - negative
+     cat(n, "tweets:", positive, "% positive,",
+         negative, "% negative,", neutral, "% neutral")
+ }
> 
> # applying classifier function
> classifier(text, pos.words, neg.words)
600 tweets: 25 % positive, 27 % negative, 48 % neutral

# Loading tweets I will use
> tweets <- parseTweets("tweets_nprnews.json")
600 tweets have been parsed. 
> lexicon <- read.csv("lexicon.csv", stringsAsFactors=F)
> pos.words <- lexicon$word[lexicon$polarity=="positive"]
> neg.words <- lexicon$word[lexicon$polarity=="negative"]

> # function to clean the text
> clean_tweets <- function(text){
+     # loading required packages
+     lapply(c("tm", "Rstem", "stringr"), require, c=T, q=T)
+     # avoid encoding issues by dropping non-unicode characters
+     utf8text <- iconv(text, to='UTF-8-MAC', sub = "byte")
+     # remove punctuation and convert to lower case
+     words <- removePunctuation(utf8text)
+     words <- tolower(words)
+     # spliting in words
+     words <- str_split(words, " ")
+     return(words)
+ }
> 
> # now I clean the text
> text <- clean_tweets(tweets$text)

> # a function to classify individual tweets
> classify <- function(words, pos.words, neg.words){
+     # count number of positive and negative word matches
+     pos.matches <- sum(words %in% pos.words)
+     neg.matches <- sum(words %in% neg.words)
+     return(pos.matches - neg.matches)
+ }
> 
> # this is how I would apply it
> classify(text[[1]], pos.words, neg.words)
[1] 0
> classify(text[[7]], pos.words, neg.words)
[1] -1
> 
> # but I want to aggregate over many tweets...
> classifier <- function(text, pos.words, neg.words){
+     # classifier
+     scores <- unlist(lapply(text, classify, pos.words, neg.words))
+     n <- length(scores)
+     positive <- as.integer(length(which(scores>0))/n*100)
+     negative <- as.integer(length(which(scores<0))/n*100)
+     neutral <- 100 - positive - negative
+     cat(n, "tweets:", positive, "% positive,",
+         negative, "% negative,", neutral, "% neutral")
+ }
> 
> # applying classifier function
> classifier(text, pos.words, neg.words)
600 tweets: 28 % positive, 24 % negative, 48 % neutral

# Loading tweets I will use
> tweets <- parseTweets("tweets_nytimes.json")
600 tweets have been parsed. 
> lexicon <- read.csv("lexicon.csv", stringsAsFactors=F)
> pos.words <- lexicon$word[lexicon$polarity=="positive"]
> neg.words <- lexicon$word[lexicon$polarity=="negative"]

> # function to clean the text
> clean_tweets <- function(text){
+     # loading required packages
+     lapply(c("tm", "Rstem", "stringr"), require, c=T, q=T)
+     # avoid encoding issues by dropping non-unicode characters
+     utf8text <- iconv(text, to='UTF-8-MAC', sub = "byte")
+     # remove punctuation and convert to lower case
+     words <- removePunctuation(utf8text)
+     words <- tolower(words)
+     # spliting in words
+     words <- str_split(words, " ")
+     return(words)
+ }
> 
> # now I clean the text
> text <- clean_tweets(tweets$text)

> # a function to classify individual tweets
> classify <- function(words, pos.words, neg.words){
+     # count number of positive and negative word matches
+     pos.matches <- sum(words %in% pos.words)
+     neg.matches <- sum(words %in% neg.words)
+     return(pos.matches - neg.matches)
+ }
> 
> # this is how I would apply it
> classify(text[[1]], pos.words, neg.words)
[1] 0
> classify(text[[7]], pos.words, neg.words)
[1] 3
> 
> # but I want to aggregate over many tweets...
> classifier <- function(text, pos.words, neg.words){
+     # classifier
+     scores <- unlist(lapply(text, classify, pos.words, neg.words))
+     n <- length(scores)
+     positive <- as.integer(length(which(scores>0))/n*100)
+     negative <- as.integer(length(which(scores<0))/n*100)
+     neutral <- 100 - positive - negative
+     cat(n, "tweets:", positive, "% positive,",
+         negative, "% negative,", neutral, "% neutral")
+ }
> 
> # applying classifier function
> classifier(text, pos.words, neg.words)
600 tweets: 22 % positive, 28 % negative, 50 % neutral

# Loading tweets I will use
> tweets <- parseTweets("tweets_usatoday.json")
600 tweets have been parsed. 
> lexicon <- read.csv("lexicon.csv", stringsAsFactors=F)
> pos.words <- lexicon$word[lexicon$polarity=="positive"]
> neg.words <- lexicon$word[lexicon$polarity=="negative"]

> # function to clean the text
> clean_tweets <- function(text){
+     # loading required packages
+     lapply(c("tm", "Rstem", "stringr"), require, c=T, q=T)
+     # avoid encoding issues by dropping non-unicode characters
+     utf8text <- iconv(text, to='UTF-8-MAC', sub = "byte")
+     # remove punctuation and convert to lower case
+     words <- removePunctuation(utf8text)
+     words <- tolower(words)
+     # spliting in words
+     words <- str_split(words, " ")
+     return(words)
+ }
> 
> # now I clean the text
> text <- clean_tweets(tweets$text)

> # a function to classify individual tweets
> classify <- function(words, pos.words, neg.words){
+     # count number of positive and negative word matches
+     pos.matches <- sum(words %in% pos.words)
+     neg.matches <- sum(words %in% neg.words)
+     return(pos.matches - neg.matches)
+ }
> 
> # this is how I would apply it
> classify(text[[1]], pos.words, neg.words)
[1] -1
> classify(text[[7]], pos.words, neg.words)
[1] -1
> 
> # but I want to aggregate over many tweets...
> classifier <- function(text, pos.words, neg.words){
+     # classifier
+     scores <- unlist(lapply(text, classify, pos.words, neg.words))
+     n <- length(scores)
+     positive <- as.integer(length(which(scores>0))/n*100)
+     negative <- as.integer(length(which(scores<0))/n*100)
+     neutral <- 100 - positive - negative
+     cat(n, "tweets:", positive, "% positive,",
+         negative, "% negative,", neutral, "% neutral")
+ }
> 
> # applying classifier function
> classifier(text, pos.words, neg.words)
600 tweets: 24 % positive, 27 % negative, 49 % neutral

# Loading tweets I will use
> tweets <- parseTweets("tweets_wapo.json")
600 tweets have been parsed. 
> lexicon <- read.csv("lexicon.csv", stringsAsFactors=F)
> pos.words <- lexicon$word[lexicon$polarity=="positive"]
> neg.words <- lexicon$word[lexicon$polarity=="negative"]

> # function to clean the text
> clean_tweets <- function(text){
+     # loading required packages
+     lapply(c("tm", "Rstem", "stringr"), require, c=T, q=T)
+     # avoid encoding issues by dropping non-unicode characters
+     utf8text <- iconv(text, to='UTF-8-MAC', sub = "byte")
+     # remove punctuation and convert to lower case
+     words <- removePunctuation(utf8text)
+     words <- tolower(words)
+     # spliting in words
+     words <- str_split(words, " ")
+     return(words)
+ }
> 
> # now I clean the text
> text <- clean_tweets(tweets$text)

> # a function to classify individual tweets
> classify <- function(words, pos.words, neg.words){
+     # count number of positive and negative word matches
+     pos.matches <- sum(words %in% pos.words)
+     neg.matches <- sum(words %in% neg.words)
+     return(pos.matches - neg.matches)
+ }
> 
> # this is how I would apply it
> classify(text[[1]], pos.words, neg.words)
[1] 0
> classify(text[[7]], pos.words, neg.words)
[1] -1
> 
> # but I want to aggregate over many tweets...
> classifier <- function(text, pos.words, neg.words){
+     # classifier
+     scores <- unlist(lapply(text, classify, pos.words, neg.words))
+     n <- length(scores)
+     positive <- as.integer(length(which(scores>0))/n*100)
+     negative <- as.integer(length(which(scores<0))/n*100)
+     neutral <- 100 - positive - negative
+     cat(n, "tweets:", positive, "% positive,",
+         negative, "% negative,", neutral, "% neutral")
+ }
> 
> # applying classifier function
> classifier(text, pos.words, neg.words)
600 tweets: 27 % positive, 26 % negative, 47 % neutral

# Loading tweets I will use
> tweets <- parseTweets("tweets_wsj.json")
600 tweets have been parsed. 
> lexicon <- read.csv("lexicon.csv", stringsAsFactors=F)
> pos.words <- lexicon$word[lexicon$polarity=="positive"]
> neg.words <- lexicon$word[lexicon$polarity=="negative"]

> # function to clean the text
> clean_tweets <- function(text){
+     # loading required packages
+     lapply(c("tm", "Rstem", "stringr"), require, c=T, q=T)
+     # avoid encoding issues by dropping non-unicode characters
+     utf8text <- iconv(text, to='UTF-8-MAC', sub = "byte")
+     # remove punctuation and convert to lower case
+     words <- removePunctuation(utf8text)
+     words <- tolower(words)
+     # spliting in words
+     words <- str_split(words, " ")
+     return(words)
+ }
> 
> # now I clean the text
> text <- clean_tweets(tweets$text)

> # a function to classify individual tweets
> classify <- function(words, pos.words, neg.words){
+     # count number of positive and negative word matches
+     pos.matches <- sum(words %in% pos.words)
+     neg.matches <- sum(words %in% neg.words)
+     return(pos.matches - neg.matches)
+ }
> 
> # this is how I would apply it
> classify(text[[1]], pos.words, neg.words)
[1] 0
> classify(text[[7]], pos.words, neg.words)
[1] 2
> 
> # but I want to aggregate over many tweets...
> classifier <- function(text, pos.words, neg.words){
+     # classifier
+     scores <- unlist(lapply(text, classify, pos.words, neg.words))
+     n <- length(scores)
+     positive <- as.integer(length(which(scores>0))/n*100)
+     negative <- as.integer(length(which(scores<0))/n*100)
+     neutral <- 100 - positive - negative
+     cat(n, "tweets:", positive, "% positive,",
+         negative, "% negative,", neutral, "% neutral")
+ }
> 
> # applying classifier function
> classifier(text, pos.words, neg.words)
600 tweets: 28 % positive, 21 % negative, 51 % neutral
